{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle, random\n",
    "import torch, os, pytorch_lightning as pl, glob\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = MedMCQA_Datamodule_to_save_datasets()\n",
    "# dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_together = load_dataset(\"openlifescienceai/medmcqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_together[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del generation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompts from dataset\n",
    "\n",
    "A,B,C,D = dataset['opa'], dataset['opb'],dataset['opc'],dataset['opd']\n",
    "questions = dataset['question']\n",
    "subject_names = dataset['subject_name']\n",
    "topic_names = dataset['topic_name']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts_long = list(map(lambda a,b,c,d,q,subject,topic: f\"Subject: {subject}, Topic: {topic}\\nQuestion: {q}\\nA: {a}\\nB: {b}\\nC: {c}\\nD: {d}\\nKey concepts: \",\n",
    "                    A,B,C,D,questions, subject_names, topic_names))\n",
    "\n",
    "input_prompts = list(map(lambda a,b,c,d,q,subject,topic: f\"Subject: {subject}, Topic: {topic}\\nQuestion: {q}\\n To answer this question, we need to notice that \",\n",
    "                    A,B,C,D,questions, subject_names, topic_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts_short = list(map(lambda q: f\"{q} It is relavent to\",\n",
    "                    questions))\n",
    "\n",
    "\n",
    "\n",
    "input_prompts_concept = list(map(lambda q: f\"{q} Key Concept:\",\n",
    "                    questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import torch\n",
    "# prompt = [\"NMR is used to\", \n",
    "#           \"Which of the following is not true about glomerular capillaries: It is relavent to\",\n",
    "#           \"A 29 yrs old woman with a pregnancy of 17 week has a 10 years old boy with down syndrome. She does not want another down syndrome kid; best advice to her is: It is relavent to\"]\n",
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"healx/gpt-2-pubmed-medium\"\n",
    "max_length = 100\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side='left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets_with_prompt_and_save(dataset, save_path):\n",
    "    \n",
    "    questions = dataset['question']\n",
    "    input_prompts_hint = list(map(lambda q: f\"{q} Hint:\",\n",
    "                    questions))\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(input_prompts_hint)//16)):\n",
    "        inputs = tokenizer(input_prompts_hint[16*i:16*(i+1)], return_tensors=\"pt\",  padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate text by feeding the encoded input through the model and sampling output tokens\n",
    "        outputs = model.generate(input_ids=inputs[\"input_ids\"],\n",
    "                                attention_mask=inputs[\"attention_mask\"], \n",
    "                                max_length=max_length+inputs[\"input_ids\"].shape[1], \n",
    "                                num_beams=10, early_stopping=True,\n",
    "                                repetition_penalty=4.2,\n",
    "                                pad_token_id=50256\n",
    "                                )\n",
    "        generated_text = tokenizer.batch_decode(outputs, \n",
    "                                        skip_special_tokens=True\n",
    "                                        )\n",
    "        generated_text = generated_text.split('.', 1)[0] + '.'\n",
    "        break\n",
    "\n",
    "\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "# # Decode the generated tokens back into textf\n",
    "# for i in range(len(outputs)):\n",
    "#     \n",
    "    \n",
    "\n",
    "\n",
    "#     print(f\"Generated Text:\\n{generated_text}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minputs\u001b[49m\u001b[38;5;241m.\u001b[39mkeys(), inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, output\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "inputs.keys(), inputs[\"input_ids\"].shape, inputs[\"attention_mask\"].shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4183"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "generation_model.tokenizer.padding_side='left'\n",
    "if generation_model.tokenizer.pad_token is None:\n",
    "    generation_model.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    generation_model.model.resize_token_embeddings(len(generation_model.tokenizer))\n",
    "else:\n",
    "    print(generation_model.tokenizer.pad_token)\n",
    "    \n",
    "generation_model.tokenizer.pad_token = generation_model.tokenizer.eos_token\n",
    "inputs  = generation_model.tokenizer(input_prompts[0:16], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "output_sequences = generation_model.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            # attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64 + inputs[\"input_ids\"].shape[1],\n",
    "            pad_token_id=generation_model.model.config.eos_token_id,  # Ensure pad_token_id is set for open-ended generation\n",
    "            temperature=1.0,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=5.2,\n",
    "            do_sample=True,\n",
    "            # num_beams  = 4,\n",
    "            num_return_sequences=4,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_model.tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 124]), torch.Size([16, 60]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences.shape, inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true for myelinated nerve fibers:\n",
      " To answer this question, we need to notice the fact that ery [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]. [PAD]    [PAD] [PAD]  * [PAD] [PAD]  ( [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ). [PAD]   [PAD] [PAD]   [PAD]  ) [PAD]  E [PAD] [PAD] [PAD] [PAD] [PAD]  / [PAD]  ; [PAD]  \n",
      "\n",
      "Generated Text 2: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true for myelinated nerve fibers:\n",
      " To answer this question, we need to notice the fact that  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]  ( [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]    [PAD] [PAD] [PAD] [PAD]  ) [PAD] [PAD] [PAD] [PAD] [PAD]   [PAD] [PAD]\n",
      "\n",
      "Generated Text 3: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true for myelinated nerve fibers:\n",
      " To answer this question, we need to notice the fact that  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]. [PAD] [PAD] [PAD] [PAD] [PAD]   [PAD] [PAD] [PAD] [PAD]   [PAD] [PAD] . [PAD]   [PAD]  should [PAD] [PAD]? [PAD]  as [PAD] [PAD]  and [PAD]\n",
      "\n",
      "Generated Text 4: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true for myelinated nerve fibers:\n",
      " To answer this question, we need to notice the fact that ery [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]. [PAD]  It [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]   [PAD] [PAD]   [PAD] [PAD]  This [PAD] [PAD] - [PAD] [PAD]  only [PAD]  do [PAD]\n",
      "\n",
      "Generated Text 5: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true about glomerular capillaries')\n",
      " To answer this question, we need to notice the fact that  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '. [PAD] [PAD] [PAD]    [PAD]  in [PAD]  I [PAD] [PAD] [PAD]   [PAD] [PAD]   [PAD]  : [PAD] '[PAD] , [PAD] [PAD] [PAD] [PAD]  should [PAD] [PAD] .\n",
      "\n",
      "Generated Text 6: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true about glomerular capillaries')\n",
      " To answer this question, we need to notice the fact that  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]. [PAD]  Theses [PAD] [PAD] [PAD] ... [PAD] [PAD] [PAD]  : [PAD] . [PAD] [PAD]  mean [PAD] [PAD] [PAD]\n",
      "\n",
      "Generated Text 7: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true about glomerular capillaries')\n",
      " To answer this question, we need to notice the fact that ery [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]! [PAD]  e [PAD] [PAD] [PAD] [PAD] [PAD]. [PAD]  and [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]  The [PAD] [PAD]  u [PAD] [PAD] '[PAD] [PAD] [PAD]  do [PAD]   ( [PAD] . [PAD]  ; ) [PAD]   \n",
      "\n",
      "Generated Text 8: Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true about glomerular capillaries')\n",
      " To answer this question, we need to notice the fact that   [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] . [PAD] [PAD] [PAD] [PAD] [PAD]  a [PAD] [PAD] [PAD] [PAD] , [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] , [PAD]   [PAD]. [PAD]\n",
      "\n",
      "Generated Text 9: Subject: Medicine, Topic: None\n",
      "Question: A 29 yrs old woman with a pregnancy of 17 week has a 10 years old boy with down syndrome. She does not want another down syndrome kid; best advice to her is\n",
      " To answer this question, we need to notice the fact that   [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ... [PAD] [PAD] [PAD] [PAD] [PAD]    [PAD] [PAD] [PAD] [PAD] . [PAD]   [PAD]  system [PAD] [PAD] [PAD] [PAD] [PAD]     [PAD] [PAD]  : [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]  ; [PAD] [PAD]  ,\n",
      "\n",
      "Generated Text 10: Subject: Medicine, Topic: None\n",
      "Question: A 29 yrs old woman with a pregnancy of 17 week has a 10 years old boy with down syndrome. She does not want another down syndrome kid; best advice to her is\n",
      " To answer this question, we need to notice the fact that   [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]    [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]  system [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] - [PAD] [PAD] [PAD]  ( [PAD] [PAD]\n",
      "\n",
      "Generated Text 11: Subject: Medicine, Topic: None\n",
      "Question: A 29 yrs old woman with a pregnancy of 17 week has a 10 years old boy with down syndrome. She does not want another down syndrome kid; best advice to her is\n",
      " To answer this question, we need to notice the fact that   [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]    [PAD]  . [PAD]   [PAD]  : [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]   , [PAD] [PAD] [PAD]    ; [PAD] [PAD]  non [PAD] [PAD] ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, generated_sequence in enumerate(output_sequences):\n",
    "    decoded_text = generation_model.tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "    print(f\"Generated Text {i+1}: {decoded_text}\")\n",
    "    print()\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Physiology, Topic: None\n",
      "Question: Which of the following is not true for myelinated nerve fibers:\n",
      "A: Impulse through myelinated fibers is slower than non-myelinated fibers\n",
      "B: Membrane currents are generated at nodes of Ranvier\n",
      "C: Saltatory conduction of impulses is seen\n",
      "D: Local anesthesia is effective only when the nerve is not covered by myelin sheath\n",
      "Key concepts: \n"
     ]
    }
   ],
   "source": [
    "print(input_prompts_long[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9041"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
