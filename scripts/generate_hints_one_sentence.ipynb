{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Main] using GPU : NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import tqdm\n",
    "import pickle, random\n",
    "import torch, os, pytorch_lightning as pl, glob\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "### downloading dataset:\n",
    "dataset_together = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "\n",
    "### downloading model:\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"healx/gpt-2-pubmed-medium\"\n",
    "max_length = 100\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side='left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f'[Main] using GPU : {torch.cuda.get_device_name()}')\n",
    "\n",
    "\n",
    "split = \"val\"\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 12\n",
    "max_length = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/349 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "assert(split in [\"val\", \"train\", \"test\"])\n",
    "\n",
    "split_for_dataset = split if split != \"val\" else \"validation\"\n",
    "dataset = dataset_together[split_for_dataset]\n",
    "questions = dataset['question']\n",
    "\n",
    "hint_prompt='Some hints:'\n",
    "input_prompts_hint = list(map(lambda q: f\"{q} {hint_prompt}\",questions))\n",
    "len_of_each_question = list(map(lambda x: len(x), input_prompts_hint))\n",
    "# generating hints \n",
    "questions_with_hints = []\n",
    "for i in tqdm.tqdm(range(len(input_prompts_hint)//batchsize + 1)):\n",
    "    # print(i,\" out of \", len(input_prompts_hint)//batchsize + 1)\n",
    "    begin, end = batchsize*i, min(batchsize*(i+1), len(input_prompts_hint))\n",
    "    inputs = tokenizer(input_prompts_hint[begin : end], \n",
    "                        return_tensors=\"pt\",  padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate text by feeding the encoded input through the model and sampling output tokens\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"],\n",
    "                            attention_mask=inputs[\"attention_mask\"], \n",
    "                            max_length=max_length+inputs[\"input_ids\"].shape[1], \n",
    "                            num_beams=10, early_stopping=True,\n",
    "                            repetition_penalty=4.2,\n",
    "                            pad_token_id=50256\n",
    "                            )\n",
    "    generated_text = tokenizer.batch_decode(outputs, \n",
    "                                    skip_special_tokens=True\n",
    "                                    )\n",
    "    generated_text_one_sentence = list(map(lambda x, l:x[l:].split('.')[0] , \n",
    "                                           generated_text,\n",
    "                                       len_of_each_question[begin : end]))\n",
    "    break\n",
    "\n",
    "\n",
    "    questions_with_hints+=generated_text_one_sentence\n",
    "# generated_text_one_sentence\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Nerve conduction studies (NCSs) can be used to assess peripheral nerve function',\n",
       " ' Glomerular filtration rate (eGFR) has been shown to be an important predictor of mortality in patients with chronic kidney disease (CKD)',\n",
       " ' In this case report, we would like to draw attention to the importance of early diagnosis and treatment of Down syndrome in order to improve the quality of life for both mother and child',\n",
       " ' The aim of this study was to investigate the effect of a high-fat diet (HFD) on axonal transport in rats',\n",
       " ' Insulin-like growth factor 1 (IGF-1) and IGF-binding protein 3 (IGFBP-3) have been shown to play an important role in the regulation of glucose homeostasis',\n",
       " ' A case report',\n",
       " \" The term 'H I N1 Influenza' has been used for a long time to describe influenza-like illness (ILI)\",\n",
       " \" \\u2002Kiesselbach's plexus can be divided into two groups according to its origin from internal carotid artery (ICA) or external carotid artery (ECA)\",\n",
       " ' A case report',\n",
       " ' this case report describes a newborn who was admitted to the neonatal intensive care unit (NICU) due to acute respiratory distress syndrome (ARDS)',\n",
       " ' Dental Caries Index (DCI) has been shown to be a valid and reliable tool for assessing oral health-related quality of life (OHRQoL)',\n",
       " ' An ABG should not be performed on patients who have been treated with anticoagulants for atrial fibrillation (AF) and in whom there is no contraindication to oral anticoagulation (OAC)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x, l:x[l:].split('.')[0] , \n",
    "                                           generated_text,\n",
    "                                       len_of_each_question[begin : end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Which of the following is not true for myelinated nerve fibers:',\n",
       " \"Which of the following is not true about glomerular capillaries')\",\n",
       " 'A 29 yrs old woman with a pregnancy of 17 week has a 10 years old boy with down syndrome. She does not want another down syndrome kid; best advice to her is',\n",
       " 'Axonal transport is:',\n",
       " 'Low insulin to glucagon ratio is seen in all of these except:',\n",
       " 'Concentration of tropicamide:',\n",
       " 'Which of the following statements is true regarding H I N1 Influenza?',\n",
       " \"Which of the following are not a branch of external carotid Aery in Kiesselbach's plexus.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "hints = questions_with_hints\n",
    "# tokenize everything\n",
    "A,B,C,D = dataset['opa'], dataset['opb'],dataset['opc'],dataset['opd']\n",
    "subject_names = dataset['subject_name']\n",
    "topic_names = dataset['topic_name']\n",
    "\n",
    "labels = dataset['cop']\n",
    "\n",
    "finetuning_tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext')\n",
    "    \n",
    "input_prompts = list(map(lambda a,b,c,d,q,h,subject,topic: f\"Subject: {subject}, Topic: {topic}\\nQuestion: Given{h}, {q}\\nA: {a}\\nB: {b}\\nC: {c}\\nD: {d}\\n\",\n",
    "                        A,B,C,D,questions, hints, subject_names, topic_names))\n",
    "\n",
    "tokens = finetuning_tokenizer(input_prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "\n",
    "        \n",
    "data_with_hints = torch.utils.data.TensorDataset(tokens.input_ids, tokens.attention_mask, torch.tensor(labels))\n",
    "torch.save(data_with_hints, '/root/pubmedQA_291/dataset_pickles/medmcqa/hints-added-tokenized-by-pubmed-bert/classification_style/'+split+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
